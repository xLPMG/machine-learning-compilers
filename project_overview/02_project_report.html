

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Project Report &mdash; Machine Learning Compilers  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="User Guide" href="03_user_guide.html" />
    <link rel="prev" title="Introduction" href="01_project_information.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Machine Learning Compilers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Project Overview</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="01_project_information.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Project Report</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_docu_setup.html">Documentation Setup</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Submissions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../submissions/01_assembly.html">1. Assembly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/02_base.html">2. Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/03_neon.html">3. Neon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/04_code_gen.html">4. Code Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/05_tensor_op.html">5. Tensor Operation Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/06_einsum.html">6. Einsum Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/07_individual_phase.html">7. Individual Phase</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit.html">mini_jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_benchmarks.html">mini_jit::benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_converters.html">mini_jit::converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_einsum.html">mini_jit::einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_instructions.html">mini_jit::instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_ir.html">mini_jit::ir</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_kernels.html">mini_jit::kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_registers.html">mini_jit::registers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Learning Compilers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Project Report</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/project_overview/02_project_report.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="project-report">
<span id="id1"></span><h1>Project Report<a class="headerlink" href="#project-report" title="Link to this heading"></a></h1>
<p>In the first two weeks of our project, we focused on ARM AArch64 assembly to build a solid foundation for understanding machine learning compilers.
During the first week, we examined the assembly output of a simple C program compiled with both <strong>GCC</strong> and <strong>Clang</strong> to explore the differences in code generation and function call conventions.
This initial exploration helped us become familiar with compiler behavior, instruction-level representations, and low-level debugging tools such as <strong>GDB</strong>.
Further details about the specific steps and tasks can be found in the <a class="reference internal" href="../submissions/01_assembly.html#assembly"><span class="std std-ref">assembly section</span></a> of our project documentation.</p>
<p>In the second week, we began writing assembly programs from scratch using only base instructions.
Specifically, we reimplemented two simple C functions in AArch64 assembly to better understand data movements and control flow at the instruction level.
After successfully replicating the functionality of the C programs, we developed microbenchmarks to evaluate the <strong>throughput</strong> and <strong>latency</strong> of key instructions such as <code class="docutils literal notranslate"><span class="pre">ADD</span></code> and <code class="docutils literal notranslate"><span class="pre">MUL</span></code>.
These benchmarks helped us gain insight into the performance characteristics of modern ARM processors and how instruction-level behavior can impact overall computation speed.
Further details about the specific steps and tasks can be found in the <a class="reference internal" href="../submissions/02_base.html#base"><span class="std std-ref">base section</span></a> of our project documentation.</p>
<p>After spending the first two weeks experimenting with base instructions and writing simple assembly programs, we advanced to working with <strong>Neon</strong> (<strong>Advanced SIMD</strong>) instructions.
In the following weeks, we explored the performance characteristics of Neon operations, an essential step toward mastering the fundamentals required for building our own tensor compiler.</p>
<p>We began by benchmarking the <a class="reference internal" href="../submissions/03_neon.html#throughput-latency"><span class="std std-ref">throughput and latency</span></a> of the <code class="docutils literal notranslate"><span class="pre">FMLA</span></code> and <code class="docutils literal notranslate"><span class="pre">FMADD</span></code> instructions.
This helped us understand the significance of instruction-level parallelism and how much instruction ordering and data dependencies can impact performance.
After these initial experiments, we shifted our focus toward understanding the role of <strong>microkernels</strong>.
We explored key design considerations, like data reuse, register allocation, and memory access patterns, and conducted our first experiments with optimizing <a class="reference internal" href="../submissions/03_neon.html#microkernel"><span class="std std-ref">microkernels</span></a> for performance.</p>
<p>In the following week, we extended our microkernel by wrapping it in <strong>loops</strong> to support larger matrix dimensions and improve the performance.
Starting from our base kernel of <code class="docutils literal notranslate"><span class="pre">16x6x1</span></code>, we progressively scaled it to handle matrices of size <code class="docutils literal notranslate"><span class="pre">64x48x64</span></code>.
This allowed us to reach the architectural performance limits of a M4 Chip.
Further implementation details can be found in the <a class="reference internal" href="../submissions/03_neon.html#loops"><span class="std std-ref">loops section</span></a> of our project documentation.</p>
<p>After exploring ideal kernel sizes aligned with our vector register widths, we also experimented with cases where the <code class="docutils literal notranslate"><span class="pre">M</span></code> dimension is not a multiple of 4 or 16.
In these scenarios, special handling was required to maintain correctness and efficiency.
We implemented and optimized dedicated kernels for two such cases, which are documented in detail in the <a class="reference internal" href="../submissions/03_neon.html#simd"><span class="std std-ref">SIMD Lanes</span></a> section of our documentation.
In the same week, we also explored the impact of <strong>accumulator block</strong> shapes for performance reasons.
Specifically, we implemented a microkernel for computing <code class="docutils literal notranslate"><span class="pre">C+=AB</span></code> with dimensions <code class="docutils literal notranslate"><span class="pre">M=64</span></code>, <code class="docutils literal notranslate"><span class="pre">N=64</span></code>, and <code class="docutils literal notranslate"><span class="pre">K=64</span></code>.
This required adapting our existing <code class="docutils literal notranslate"><span class="pre">matmul_64_48_64</span></code> kernel to support the extended <code class="docutils literal notranslate"><span class="pre">N</span></code> dimension.
The details and benchmarking results of this extension are documented in the <a class="reference internal" href="../submissions/03_neon.html#accumulator-block-shapes"><span class="std std-ref">Accumulator Block Shapes</span></a> section.</p>
<p>After implementing and optimizing standard matrix multiplication kernels, we extended our work to support <a class="reference internal" href="../submissions/03_neon.html#batch-reduce-gemm"><span class="std std-ref">batch-reduce GEMM</span></a> operations.
We reused and adapted our existing microkernels to handle <strong>batches of matrices</strong> efficiently.</p>
<p>The last part of the Neon section was to explore how to <strong>transpose</strong> matrices using Neon assembly.
Our goal was to handle a <code class="docutils literal notranslate"><span class="pre">8x8</span></code> matrix, which we handled by dividing it into four <code class="docutils literal notranslate"><span class="pre">4x4</span></code> submatrices.
More details and code can be found in the <a class="reference internal" href="../submissions/03_neon.html#transposition"><span class="std std-ref">transposition section</span></a> of our documentation.</p>
<p>In week 4, we turned our attention to code generation.
The idea was to generate the previously implemented Neon assembly kernels using C++ during runtime.
Starting with a rather simple example, we began by implementing a <code class="docutils literal notranslate"><span class="pre">matmul_16_6_k</span></code> <a class="reference internal" href="../submissions/04_code_gen.html#brgemm-microkernel"><span class="std std-ref">microkernel</span></a>.
For this, we learned how to generate assembly instructions by setting each bit manually, writing the 32-bit instructions to previously allocated memory and lastly making that memory executable.</p>
<p>The next task was to extend the <code class="docutils literal notranslate"><span class="pre">matmul_16_6_k</span></code> by generating loops over the <code class="docutils literal notranslate"><span class="pre">M</span></code> and <code class="docutils literal notranslate"><span class="pre">N</span></code> dimensions, resulting in a <strong>GEMM</strong> kernel.
This also involved writing a backend entry point for kernel generation and unit tests for verification.
After verifying that the GEMM kernel generation worked as intended, we proceeded with implementing a <strong>Batch-Reduce GEMM (BRGEMM)</strong> kernel.
Here too, we verified our implementation using unit tests.
Lastly, we performed an extensive benchmark of the GEMM and BRGEMM kernels for different matrix dimensions.
In total, this benchmark took over 8 hours on the provided Apple M4 machine.
For more information, refer to <a class="reference internal" href="../submissions/04_code_gen.html#brgemm-primitive"><span class="std std-ref">4.1 BRGEMM Primitive</span></a>.</p>
<p>In week 5, we extended our code generator by <strong>Unary Primitives</strong> of the form B:=op(A).
Similarly to the BRGEMM backend, we first implemented a new entry point which can be used to generate various unary primitive kernels.
The first unary primitive we implemented was the <strong>Zero Primitive</strong>, which sets all elements of the output tensor to zero.
Secondly, we implemented the <strong>Identity Primitive</strong> which copies all elements of the input tensor to the output tensor.
The complicated part here was to support transposition for arbitrary tensor sizes.
Lastly, we implemented an activation function commonly found in machine learning frameworks: the <strong>ReLU Primitive</strong>.
This operation sets all negative elements to zero and keeps positive elements as they are.
For all implemented unary operations we implemented unit tests and benchmarked the performance.
Further information can be found in <a class="reference internal" href="../submissions/04_code_gen.html#unary-primitives"><span class="std std-ref">4.2 Unary Primitives</span></a>.</p>
<p>In week 6 we received the task of developing a <strong>Tensor Operation Backend</strong>, see <a class="reference internal" href="../submissions/05_tensor_op.html#tensor-op-backend"><span class="std std-ref">5. Tensor Operation Backend</span></a>.
This backend not only <strong>sets up and holds the kernel objects</strong>,
but also <strong>blocks the input and output tensors</strong> and executes the kernels accordingly.
We started with a verification of all input parameters and then implemented an <code class="docutils literal notranslate"><span class="pre">execute</span></code> function,
which calls itself recursively to work through the nested sequential loops of an input tensor.
To end this week’s task, we performed extensive benchmarks with various configuration parameters.</p>
<p>In the following week, we added support for <strong>Shared Memory Parallelization</strong>.
This meant that we had to check whether the input tensor contained any dimensions that should be executed in parallel.
If that was the case, we flattened all shared dimensions into one big iteration space and then parallelized it using <strong>OpenMP</strong>.
You can find more information <a class="reference internal" href="../submissions/05_tensor_op.html#shared-memory-parallelization"><span class="std std-ref">here</span></a>.</p>
<p>The second task of week 7 was to implement optimization passes, see <a class="reference internal" href="../submissions/05_tensor_op.html#optimization-passes"><span class="std std-ref">5.5 Optimization Passes</span></a>.
First, we developed an intermediate representation of the tensors consisting of Dimension <code class="docutils literal notranslate"><span class="pre">struct</span></code>’s.
Then, we applied <strong>Primitive Identification</strong>, <strong>Dimension Splitting</strong> and the <strong>Parallelization of Sequential Dimensions</strong> to the vectors of the Dimension <code class="docutils literal notranslate"><span class="pre">struct</span></code>’s that represent each input tensor.</p>
<p>In week 8, we enhanced our tensor operation backend and our optimizer by supporting <strong>Unary Operations</strong>, such as permuting a tensor’s dimensions.
Here, we first had to verify that all dimensions of such an operation are of type <code class="docutils literal notranslate"><span class="pre">C</span></code> and appear both in the input and the output tensor. Additionally, the second input tensor had to be automatically set to <code class="docutils literal notranslate"><span class="pre">nullptr</span></code>, as unary operations only support one input.
Next, we had to implement new primitive identification and shared memory parallelization optimization passes for the unary operations and finally verify the correctness of our code against a reference implementation.
To view the results and implementations, visit our <a class="reference internal" href="../submissions/05_tensor_op.html#unary-operations"><span class="std std-ref">detailed report</span></a>.</p>
<p>In the final weekly submission of our project, we made important enhancements to the flexibility of our tensor compiler.
The first major task was the support for <strong>Einsum Tree Expressions</strong>.
We began with a <a class="reference internal" href="../submissions/06_einsum.html#einsum-parsing"><span class="std std-ref">string parser</span></a> that transforms expressions of the form <code class="docutils literal notranslate"><span class="pre">[...],[...]-&gt;[...]</span></code> into an internal tree representation. The next step involved <a class="reference internal" href="../submissions/06_einsum.html#einsum-lowering-subchapter"><span class="std std-ref">lowering this einsum tree to our tensor operation backend</span></a>. We then applied our existing <a class="reference internal" href="../submissions/06_einsum.html#einsum-node-optimizations"><span class="std std-ref">optimization passes</span></a> on this tree structure, and finally ensured that the tree could be correctly <a class="reference internal" href="../submissions/06_einsum.html#einsum-execution"><span class="std std-ref">executed</span></a>. To assess the correctness and performance of our implementation, we ran a series of benchmarks. A comprehensive report on this work is available in <a class="reference internal" href="../submissions/06_einsum.html#einsum-lowering"><span class="std std-ref">6.1 Lowering</span></a>.</p>
<p>The second major task was to implement <strong>Optimization Passes for Einsum Trees</strong>.
These differ from the previous optimization which focused on the internals of a single tensor operation.
In this task, we looked at <strong>Node Swapping</strong>, <strong>Dimension Reordering</strong> and the <strong>Insertion of Permutation Nodes</strong>.
These tree optimizations come before the initialization of each tree node, and reshape the einsum tree to improve performance.
Refer to <a class="reference internal" href="../submissions/06_einsum.html#einsum-tree-optimizations"><span class="std std-ref">6.2 Einsum Tree Optimization</span></a> to learn more about our implementations.</p>
<p>The last task was to propose a pitch and write a short sketch on the individual phase that would conclude this project.
To read our pitch and sketch, please take a look at <a class="reference internal" href="../submissions/07_individual_phase.html#individual-phase"><span class="std std-ref">7. Individual Phase</span></a>.</p>
<p>During the last two weeks, we had to fulfill the pitch that we had written in the prior week.
We used the first week of the individual phase to extend the flexibility of our tensor compiler by implementing various unary and binary primitives.
In the second week, we put our attention on the rather complex sigmoid activation function and added multiple implementations with different speeds and accuracies.
Lastly, we improved our optimizer by adding support for <strong>Dimension Fusion</strong>.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01_project_information.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="03_user_guide.html" class="btn btn-neutral float-right" title="User Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Lucas Obitz, Luca-Philipp Grumbach.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>